---
title: "Announcing cudarc and GPU accelerated dfdx: ergonomic deep learning ENTIRELY in rust!"
subtitle: "Super fast, compile AND runtime shaped tensors, tiny executables, and more."
---

# Table of contents

* TOC
{:toc}

# What is dfdx?

An ergonomic deep learning library in rust (similar to pytorch in python). `dfdx` has all the standard tensor operations, neural network modules, and optimizers. It also knows tensor sizes across all operations, meaning if you use compile time dimensions, you get shape checking for free! No more kicking off a training run and getting a runtime shape error. It's easy to use, and the internals are much easier to understand something like pytorch or tensorflow.

Check out [dfdx/examples](TODO) for more examples, but here is an api preview:

```rust
// Declare our model structure using tuples to represent sequential models
type Model = (
    (Linear<5, 10>, ReLU),
    Residual<(Linear<10, 10>, Tanh)>,
    Linear<10, 2>,
);

// We'll need a device to allocate tensors/models
let dev: Cuda = Default::default();

// Create our model with f32 dtype. f64 is also supported (f16 is coming in the future!)
let mut model = dev.build_module::<Model, f32>();

// Use Adam as our model optimizers
let mut opt = Adam::new(&model, AdamConfig {
    lr: 3e-4,
    ..Default::default(),
});

// pre-allocate the model gradients
let mut grads = model.alloc_grads();

// here we are using runtime batch size, but you can use`Const::<10>`.
let batch_size = 10usize;

// just some random input/output data for simplicity - x will be of size `(10, 5)`,
// y will be of size `(10, 2)`
let x = dev.sample_normal_like(&(batch_size, Const::<5>));
let y = dev.sample_normal_like(&(batch_size, Const::<2>)).softmax();

// do a forward pass - traced takes ownership of our gradients
let pred = model.forward_mut(x.traced(grads));
let loss = cross_entropy_with_logits_loss(pred, y);

// do a backward pass to take the gradients back
grads = loss.backward();

// update our model using the gradients we computed,
// and then reset them!
opt.update(&mut model, &grads).unwrap();
model.zero_grads(&mut grads);
```

# What is cudarc?

`cudarc` is a safe rust wrapper around the CUDA toolkit. It aims to be as safe as possible given interacting with CUDA involves all FFI calls. `dfdx` leverages `cudarc` for all of it's CUDA support.

Check out [cudarc/examples](TODO) for more examples, but here is an api preview:

```rust
// Easy creation of devices - this creates a handle to device id 0
let dev = CudaDevice::new(0)?;

// You can load a function from a pre-compiled PTX like so:
dev.load_ptx(Ptx::from_file("./examples/sin.ptx"), "sin", &["sin_kernel"])?;

// and then retrieve the function with `get_func`
let f = dev.get_func("sin", "sin_kernel").unwrap();

// copy vecs to the device
let a_dev = dev.htod_copy(std::vec![1.0, 2.0, 3.0])?;
let mut b_dev = a_dev.clone();

// and launch kernels!
let cfg = LaunchConfig::for_num_elems(3);
unsafe { f.launch(cfg, (&mut b_dev, &a_dev, 3i32)) }?;
```

# dfdx new features at a glance

## Tensor is now generic over Shape, Dtype, and Device

You can now represent tensors with different:
- Shapes, including mixed compile time and runtime dimensions
- Dtypes, (both f32/f64 are supported right now)
- Different device storages (Cuda & Cpu)

Here are some example tensors:

```rust
// a 3d tensor of shape (1, 2, 3), storing f32s
Tensor<Rank3<1, 2, 3>, f32, Cpu>

// a 2d tensor with runtime shape storing f64s on the Cuda device
Tensor<(usize, usize), f64, Cuda>

// a 3d tensor with shape (3, ?, 5) storing `usize` on the Cuda device
Tensor<(Const<3>, usize, Const<5>), usize, Cuda>

// a 1d tensor with shape (10,) storing `u16` on Cuda
Tensor<Rank1<10>, u16, Cuda>

// a 0d tensor storing a boolean value
Tensor<(), bool, Cuda>
```

And for those curious, check out the internals of the struct [here](TODO) (hint: we use GATs!).

Checkout [dfdx/examples/01-tensor.rs](TODO) for more usage examples.

## Cuda device

This release introduced both the `Cpu` and `Cuda` device. Both support all operations/apis in dfdx, so its easy to switch between them!

```diff
- type Device = Cpu;
+ type Device = Cuda;
let dev: Device = Default::default();
let x: Tensor<Rank2<2, 3>, f32, _> = dev.zeros();
let m = dev.build_module::<Linear<5, 10>, f32>;
```

## Gradient accumuluation

Gradient accumulation is often used to train on a bigger batch size than you can actually run on. This is trivial to implement now thanks to some updates to the tracing and gradients APIs.

```rust
let pred = model.forward_mut(x1.traced(grads));
grads = loss(pred, truth).backward()

// accumulate twice
let pred = model.forward_mut(x2.traced(grads));
grads = loss(pred, truth).backward()

// accumulate a third time
let pred = model.forward_mut(x3.traced(grads));
grads = loss(pred, truth).backward()
```

## Exponential moving average of neural networks

A common technique in many domains of DL is to keep a second model which is an exponential moving average of your main model. The update formula is looks like `model_ema = model_ema * decay + model * (1 - decay)`.

With some new internal techniques, we are able to get this for free on any module that implements our `trait TensorCollection`!

```rust
let mut model = dev.build_module::<Model, f32>();
let mut model_ema = dev.build_module::<Model, f32>();
model_ema.ema(&model, 0.001);
```

## Dataset/DataLoader utilities

I've added a number of iterator extension methods to take the place of dataloaders from other frameworks. One of the best parts is you can use these with normal iterators too! Nothing super special for tensors.

Here's how they all work together (this is ripped from the `examples/06-mnist.rs`):

```rust
let mut preprocess = |(img, lbl)| { ... };

dataset
    // from implementing dfdx::data::ExactSizeDataset
    .shuffled(&mut rng)

    // just a normal iterator map
    .map(preprocess)

    // create batches with a compile time know size - this creates
    // an iterator with items `[Item; BATCH_SIZE]`
    // `.batch(BATCH_SIZE)` would create items of `Vec` with runtime known size
    .batch(Const::<BATCH_SIZE>)

    // turns items like `Vec<(A, B)>` into `(Vec<A>, Vec<B>)`, but also works for array items!
    .collate()

    // turns items like `Vec<Tensor>`/`[Tensor; N]` into Tensor with a new dimension
    .stack()
```

# What's next for dfdx?

Here's a high level view of what's on my roadmap for new features in dfdx (you can also checkout the issues page for more insight):

- float16 and AMP dtypes
- Distributed Gpu/Cpu devices
- Lazy disk tensors
- CUDA & CPU kernel optimizations
- Multi threaded CPU kernels
- ONNX serialization

In parallel, I'll build out separate crates for the following applications:

- Image classification (resnets)
- Text generation (gpt2/llama)
- Text to image (stable diffusion)
- Audio to text (whisper)

# Thanks

A big thanks to all my sponsors, including:
- TODO

Additionally a big shout out to all the contributors that helped with this release!
- TODO
