---
title: "Announcing cudarc and GPU accelerated dfdx: ergonomic deep learning ENTIRELY in rust!"
subtitle: "Super fast, compile AND runtime shaped tensors, tiny executables, and more."
---

# Table of contents

* TOC
{:toc}

# What is dfdx?

An ergonomic deep learning library in rust (similar to pytorch in python). `dfdx` has all the standard tensor operations, neural network modules, and optimizers. It also knows tensor sizes across all operations, meaning if you use compile time dimensions, you get shape checking for free! No more kicking off a training run and getting a runtime shape error. It's easy to use, and the internals are much easier to understand something like pytorch or tensorflow.

Check out [dfdx/examples](TODO) for more examples, but here is an api preview:

```rust
// Declare our model structure using tuples to represent sequential models
type Model = (
    (Linear<5, 10>, ReLU),
    Residual<(Linear<10, 10>, Tanh)>,
    Linear<10, 2>,
);

// We'll need a device to allocate tensors/models.
let dev: Cuda = Default::default();

// Create our model with f32 dtype.
// f64 is also supported (f16 is coming in the future!).
let mut model = dev.build_module::<Model, f32>();

// Just some random input/output data for simplicity.
// `x` will have shape `(10, 5)`.
let x: Tensor<(usize, Const<5>), f32, _> = dev.sample_normal_like(&(10, Const));

// A simple forward pass.
let pred = model.forward(x);
```

# What is cudarc?

`cudarc` is a safe rust wrapper around the CUDA toolkit. It aims to be as safe as possible given interacting with CUDA involves all FFI calls. `dfdx` leverages `cudarc` for all of it's CUDA support.

Check out [cudarc/examples](TODO) for more examples, but here is an api preview:

```rust
// Creates a handle to device ordinal 0.
let dev = CudaDevice::new(0)?;

// You can load a function from a pre-compiled PTX like so:
dev.load_ptx(Ptx::from_file("./examples/sin.ptx"), "sin", &["sin_kernel"])?;

// Then retrieve the function with `get_func`:
let f = dev.get_func("sin", "sin_kernel").unwrap();

// Copy some `Vec`s to the device.
let a_dev = dev.htod_copy(std::vec![1.0, 2.0, 3.0])?;
let mut b_dev = a_dev.clone();

// And finally launch the kernel.
let cfg = LaunchConfig::for_num_elems(3);
unsafe { f.launch(cfg, (&mut b_dev, &a_dev, 3i32)) }?;
```

# dfdx new features at a glance

## Cuda device

This release introduced both the `Cpu` and `Cuda` device. Both support __all__ operations/apis in dfdx, so its easy to switch between them!

```diff
- type Device = Cpu;
+ type Device = Cuda;
let dev: Device = Default::default();
let x: Tensor<Rank2<2, 3>, f32, _> = dev.zeros();
let m = dev.build_module::<Linear<5, 10>, f32>;
```

Plus the Cuda device is __absurdly__ fast.

## Tensor is now generic over Shape, Dtype, and Device

You can now represent tensors with different:
- Shapes, including mixed compile time and runtime dimensions
- Dtypes, (both f32/f64 are supported right now)
- Device storages (Cuda & Cpu)

Here are some example tensors:

```rust
// A 3d tensor of shape (1, 2, 3), storing f32s.
Tensor<Rank3<1, 2, 3>, f32, Cpu>

// A 2d CUDA tensor with shape (?, ?) storing f64s.
Tensor<(usize, usize), f64, Cuda>

// A 3d CUDA tensor with shape (3, ?, 5) storing `usize`.
Tensor<(Const<3>, usize, Const<5>), usize, Cuda>

// A 1d CUDA tensor with shape (10,) storing `u16`.
Tensor<Rank1<10>, u16, Cuda>

// A 0d tensor storing a boolean value.
Tensor<(), bool, Cpu>
```

And for those curious, check out the internals of the struct [here](TODO) (hint: we use GATs!).

Checkout [dfdx/examples/01-tensor.rs](TODO) for more usage examples.

## Gradient accumuluation

Gradient accumulation is often used to train on a bigger batch size than you can actually run on. This is trivial to implement now thanks to some updates to the tracing and gradients APIs.

```rust
// It's important to allocate the gradients when doing accumulation:
let mut grads = model.alloc_grads();

let pred = model.forward_mut(x1.traced(grads));
grads = loss(pred, truth).backward();

// Accumulate by doing the same thing again.
let pred = model.forward_mut(x2.traced(grads));
grads = loss(pred, truth).backward();

// When we are done accumulating, you can zero out the gradients:
model.zero_grads(&mut grads);
```

## Exponential moving average of neural networks

A common technique in many domains of DL is to keep a second model which is an exponential moving average of your main model. The update formula is looks like `model_ema = model_ema * decay + model * (1 - decay)`.

With some new internal techniques, we are able to get this for free on any module that implements our `trait TensorCollection`!

```rust
let mut model = dev.build_module::<Model, f32>();
let mut model_ema = dev.build_module::<Model, f32>();
model_ema.ema(&model, 0.001);
```

## Dataset/DataLoader utilities

I've added a number of iterator extension methods to take the place of dataloaders from other frameworks. One of the best parts is you can use these with normal iterators too! Nothing super special for tensors.

Here's how they all work together (this is ripped from the `examples/06-mnist.rs`):

```rust
let mut preprocess = |(img, lbl)| { ... };

dataset
    // From `dfdx::data::ExactSizeDataset`.
    .shuffled(&mut rng)

    // Just a normal iterator map.
    .map(preprocess)

    // Turns `Item` into `[Item; BATCH_SIZE]`.
    // `.batch(BATCH_SIZE)` would create `Vec<Item>`.
    .batch(Const::<BATCH_SIZE>)

    // Turns `Vec<(A, B)>` into `(Vec<A>, Vec<B>)`.
    // Also works for array items!
    .collate()

    // Turns `Vec<Tensor>`/`[Tensor; N]` into a Tensor
    // with an additional dimension
    .stack()
```

# What's next for dfdx?

Here's a high level view of what's on my roadmap for new features in dfdx (you can also checkout the issues page for more insight):

- float16 and AMP dtypes
- CUDA & CPU kernel optimizations
- Distributed Gpu/Cpu devices
- Lazy disk tensors
- Multi threaded CPU kernels
- ONNX serialization

In parallel, I'll build out separate crates for the following applications:

- Image classification (resnets)
- Text generation (gpt2/llama)
- Text to image (stable diffusion)
- Audio to text (whisper)

# Thanks

A big thanks to all my sponsors, including:
- TODO

Additionally a big shout out to all the contributors that helped with this release!
- TODO
