---
title: "dfdx v0.10.0"
---

First of all, thank you to [all my generous sponsors](https://github.com/sponsors/coreylowman/).
Any amount of support goes a long way for me, so I really appreciate it!

Various links:
1. [TODO release notes](#)
2. [github](https://github.com/coreylowman/dfdx)
3. [docs.rs/dfdx](https://docs.rs/dfdx/latest/dfdx/)
4. [crates.io/dfdx](https://crates.io/crates/dfdx)

# Table of contents

* TOC
{:toc}

# My Favorite things about the release

### Ergonomic Axis Reductions

> > Uh... What the heck is an axis reduction?
>
> It's the same as rust's [Iterator::reduce](https://doc.rust-lang.org/std/iter/trait.Iterator.html#method.reduce),
> but can be applied to a specific axis (or axes) of a tensor.
>
> An axis of a tensor refers to one of its dimensions. For example an `n`
> dimensional tensor (or array) has `n` axes.

For this release I focused on implementing the [TODO nn::BatchNorm2D](#) layer often used in convolutional networks. This actually needed quite a bit infrastructure added including reductions along multiple axes.

I also added axis permutations for re-ordering the axes of a tensors, which is
used in [TODO nn::MultiHeadAttention](#).

I went through a lot of designs, but what I ended up with feels very consistent
across broadcasting/reducing/reshape/permute. It's also really easy to use!

##### What does it look like?

You can see more detailed examples at [examples/08-tensor-broadcast-reduce.rs](https://github.com/coreylowman/dfdx/blob/main/examples/08-tensor-broadcast-reduce.rs) and [examples/09-tensor-permute.rs](https://github.com/coreylowman/dfdx/blob/main/examples/09-tensor-permute.rs), but here's a preview:

```rust
let a: Tensor1D<3> = tensor([1.0, 2.0, 3.0]);
let b: Tensor2D<5, 3> = a.broadcast();
assert_eq!(b.data(), &[[1.0, 2.0, 3.0]; 5]);
```

Note that you don't need to specify the axes here for rust to know how to broadcast the tensor. It automatically figures out that you mean to broadcast the 0th axis!

Here is a corresponding reduce by summing example:

```rust
// NOTE: b from the snippet above
let c: Tensor1D<5> = b.sum();
// NOTE: the [1.0, 2.0, 3.0] gets summed
assert_eq!(c.data(), &[6.0; 5]);
```

Again, you don't need to specify what axes to sum - **rust figures that out based on the type you specify**.

##### Why does this matter?

I think this is super ergonomic, but I also like these designs because the *code acts as documentation*.

I can't count the number of times I've seen comments in python code that say what shape the tensors are at specific points. You can see an example of this in this
[karpathy's minGPT code](https://github.com/karpathy/minGPT/blob/master/mingpt/model.py#L57):

```python
k = k.view(B, T, self.n_head, C // self.n_head).transpose(1, 2) # (B, nh, T, hs)
```

There's nothing there that's enforcing the comments accuracy, or that the tensor is actually that shape!

Here's how a [similar code snippet looks in dfdx](https://github.com/coreylowman/dfdx/blob/main/src/nn/transformer/mha.rs#L151):
```rust
let k: Tensor4D<B, S2, H, { K / H }, _> = k.reshape();
let k: Tensor4D<B, H, S2, { K / H }, _> = k.permute();
```

The shapes are enforced at compile time, and also act as documentation!

##### The traits that make this possible

###### BroadcastTo

I find the traits that all the examples are using really simple to understand (hopefully you do too).

```rust
trait BroadcastTo<T, Axes> {
    fn broadcast(self) -> T;
}
```

Here `Axes` is a type used to disambiguate implementations. There are a number of structs you can use for this type including:
- [TODO AllAxes](#)
- [TODO Axis](#)
- [TODO Axes2](#)
- [TODO Axes3](#)
- [TODO Axes4](#)

A user can also specify the axes to broadcast if they don't want to specify the output type or if it's ambiguous:

```rust
let a: Tensor1D<3> = tensor([1.0, 2.0, 3.0]);
let b: Tensor3D<3, 3, 3> = BroadcastTo::<_, Axes2<0, 2>>::broadcast(a);
```

Broadcasting is just copying data, so there is only [one snippet of code for the actual impl](https://github.com/coreylowman/dfdx/blob/main/src/tensor_ops/impl_broadcast_reduce.rs#L53), but each variant of the axes is one macro call.

###### PermuteTo

Permutations look pretty much exactly the same, though the [macros to implement](https://github.com/coreylowman/dfdx/blob/main/src/tensor_ops/permute.rs#L87) *all possible orders of axes* are pretty crazy.

```rust
pub trait PermuteTo<T, Axes> {
    fn permute(self) -> T;
}
```

###### Reduce & ReduceTo

Reductions on the other hand depend on *how* you want to reduce. So each different kind of operation gets its own function, and the trait specifies output type:

```rust
trait Reduce<Axes> {
    // The reduced type can be broadcasted back to Self - cool!
    type Reduced: BroadcastTo<Self, Axes>;
};

trait ReduceTo<T, Axes>: Reduce<Axes, Reduced = T> {}
```

One example of this is to reduce by summing all the values along axes - the [sum()](https://github.com/coreylowman/dfdx/blob/main/src/tensor_ops/impl_sum.rs#L30) function:

```rust
fn sum<T: Reduce<Axes>, Axes>(t: T) -> T::Reduced { ... }
```

##### What do they look like in other DL frameworks?

For completeness here's the simple example I showed above using pytorch in python:

```python
import torch
a = torch.tensor([1.0, 2.0, 3.0])
b = a.broadcast_to((5, 3))
c = b.sum(1)
```

### no_std support & ~~Spring~~ Fall Cleaning

Another big win was depending on [no_std_compat to enable turning off std](https://github.com/coreylowman/dfdx/pull/244). This was first asked by @antimora, so thanks for bringing this up!

It's always nice to clean things up, and as part of the no_std changes, I thinned out the features of all of dfdx's dependencies. Also improved the intel-mkl features, and even added a [TODO feature flags documentation](#) page!

### Using Arc in tensors

@caelunshun opened a [really nice PR](https://github.com/coreylowman/dfdx/pull/236) about using `Arc` instead of `Rc`

I like this change a lot because:
1. The diff is satisfyling small
2. It's big impact because now you can share tensors and things that contain tensors across threads!

### Better cloning semantics

Had a [great discussion](https://github.com/coreylowman/dfdx/issues/248) with @M1ngXU about how cloning of tensors works, and it led to some [really nice simplifications](https://github.com/coreylowman/dfdx/pull/249) of how clone is implemented.

### A unit test a day keeps the bugs away

I had to do a ton of refactors, and it would have been way more painful without all the unit tests. I encourage everyone to write more tests, your future self will definitely thank you.

Notably I changed conv2d's implementation to [use matrix multiplications](https://github.com/coreylowman/dfdx/pull/237), and the unit tests for conv2d caught quite a few issues I had during the rewrite.

# Next releases

The next few releases will be focused on adding multiple devices to dfdx (including a CUDA device). Unfortunately they will be breaking, but the current APIs don't really support multiple types of devices, so the breakages are definitely necessary.

I will discuss my plans for CUDA support in another blog post coming soon!
